{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from archetypax.models import ImprovedArchetypalAnalysis\n",
    "from archetypax.tools import ArchetypalAnalysisEvaluator, ArchetypalAnalysisInterpreter, ArchetypalAnalysisVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(2000, 30)\n",
    "\n",
    "# Train models with varying numbers of archetypes\n",
    "models = {}\n",
    "for k in range(2, 11):\n",
    "    model = ImprovedArchetypalAnalysis(n_archetypes=k)\n",
    "    model.fit(X)\n",
    "    models[k] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate interpretability\n",
    "interpreter = ArchetypalAnalysisInterpreter(models)\n",
    "interpreter.evaluate_all_models(X)\n",
    "interpreter.plot_interpretability_metrics()\n",
    "\n",
    "# Suggest optimal number of archetypes\n",
    "optimal_k = interpreter.suggest_optimal_archetypes(method=\"balance\")\n",
    "print(f\"Optimal number of archetypes: {optimal_k}\")\n",
    "\n",
    "# Examine detailed interpretability of the best model\n",
    "best_model = models[optimal_k]\n",
    "best_results = interpreter.results[optimal_k]\n",
    "print(f\"Interpretability score: {best_results['interpretability_score']:.4f}\")\n",
    "print(f\"Information gain: {best_results.get('information_gain', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 66\n",
    "\n",
    "# Create 3 clusters with some overlap\n",
    "cluster1 = np.random.randn(n_samples // 3, 2) * 0.5 + np.array([2, 2])\n",
    "cluster2 = np.random.randn(n_samples // 3, 2) * 0.5 + np.array([-2, 2])\n",
    "cluster3 = np.random.randn(n_samples // 3, 2) * 0.5 + np.array([0, -2])\n",
    "\n",
    "X = np.vstack([cluster2, cluster1, cluster3])\n",
    "for _ in range(5):\n",
    "    np.random.shuffle(X)\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "\n",
    "# Train models with varying numbers of archetypes\n",
    "models = {}\n",
    "for k in range(2, 11):\n",
    "    model = ImprovedArchetypalAnalysis(n_archetypes=k)\n",
    "    model.fit(X)\n",
    "    models[k] = model\n",
    "\n",
    "# Evaluate interpretability\n",
    "interpreter = ArchetypalAnalysisInterpreter(models)\n",
    "interpreter.evaluate_all_models(X)\n",
    "interpreter.plot_interpretability_metrics()\n",
    "\n",
    "# Suggest optimal number of archetypes\n",
    "optimal_k = interpreter.suggest_optimal_archetypes(method=\"balance\")\n",
    "print(f\"Optimal number of archetypes: {optimal_k}\")\n",
    "\n",
    "# Examine detailed interpretability of the best model\n",
    "best_model = models[optimal_k]\n",
    "best_results = interpreter.results[optimal_k]\n",
    "print(f\"Interpretability score: {best_results['interpretability_score']:.4f}\")\n",
    "print(f\"Information gain: {best_results.get('information_gain', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "visualizer = ArchetypalAnalysisVisualizer()\n",
    "visualizer.plot_loss(model)\n",
    "visualizer.plot_membership_weights(model, n_samples=10)\n",
    "visualizer.plot_archetype_distribution(model)\n",
    "visualizer.plot_archetypes_2d(model, X)\n",
    "visualizer.plot_reconstruction_comparison(model, X)\n",
    "\n",
    "if model.n_archetypes == 3:\n",
    "    visualizer.plot_simplex_2d(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = ArchetypalAnalysisEvaluator(model)\n",
    "\n",
    "# Generate comprehensive evaluation report\n",
    "evaluator.print_evaluation_report(X)\n",
    "\n",
    "# Calculate individual evaluation metrics\n",
    "explained_var = evaluator.explained_variance(X)\n",
    "purity = evaluator.dominant_archetype_purity()\n",
    "\n",
    "# Visualize high-dimensional data\n",
    "# evaluator.plot_feature_importance_heatmap(feature_names=column_names)\n",
    "evaluator.plot_archetype_feature_comparison(top_n=10)\n",
    "evaluator.plot_weight_distributions()\n",
    "evaluator.plot_distance_matrix()\n",
    "evaluator.plot_entropy_vs_reconstruction(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def generate_customer_transactions(customer_id, n_transactions, start_date, products_df, archetype_type):\n",
    "    transactions = []\n",
    "\n",
    "    if archetype_type == \"bulk_buyer\":\n",
    "        visit_frequency = max(1, int(np.random.normal(14, 3)))\n",
    "        basket_size_mean = 15\n",
    "        basket_size_std = 3\n",
    "        weekend_prob = 0.8\n",
    "        evening_prob = 0.3\n",
    "        health_prob = 0.4\n",
    "        premium_prob = 0.3\n",
    "        discount_sensitivity = 0.7\n",
    "\n",
    "    elif archetype_type == \"premium_daily\":\n",
    "        visit_frequency = max(1, int(np.random.normal(3, 1)))\n",
    "        basket_size_mean = 6\n",
    "        basket_size_std = 2\n",
    "        weekend_prob = 0.4\n",
    "        evening_prob = 0.5\n",
    "        health_prob = 0.7\n",
    "        premium_prob = 0.8\n",
    "        discount_sensitivity = 0.2\n",
    "\n",
    "    elif archetype_type == \"time_saving\":\n",
    "        visit_frequency = max(1, int(np.random.normal(4, 1)))\n",
    "        basket_size_mean = 4\n",
    "        basket_size_std = 1\n",
    "        weekend_prob = 0.2\n",
    "        evening_prob = 0.8\n",
    "        health_prob = 0.3\n",
    "        premium_prob = 0.4\n",
    "        discount_sensitivity = 0.5\n",
    "\n",
    "    else:  # 'price_sensitive'\n",
    "        visit_frequency = max(1, int(np.random.normal(7, 2)))\n",
    "        basket_size_mean = 8\n",
    "        basket_size_std = 2\n",
    "        weekend_prob = 0.6\n",
    "        evening_prob = 0.4\n",
    "        health_prob = 0.3\n",
    "        premium_prob = 0.1\n",
    "        discount_sensitivity = 0.9\n",
    "\n",
    "    current_date = start_date\n",
    "    for i in range(n_transactions):\n",
    "        days_increment = max(1, int(np.random.normal(visit_frequency, visit_frequency / 3)))\n",
    "        current_date += datetime.timedelta(days=days_increment)\n",
    "\n",
    "        is_weekend = np.random.random() < weekend_prob\n",
    "        day_of_week = np.random.choice([5, 6]) if is_weekend else np.random.randint(0, 5)\n",
    "\n",
    "        is_evening = np.random.random() < evening_prob\n",
    "        hour = np.random.randint(17, 23) if is_evening else np.random.randint(9, 17)\n",
    "\n",
    "        days_to_add = int((day_of_week - current_date.weekday()) % 7)\n",
    "        txn_date = current_date + datetime.timedelta(days=days_to_add)\n",
    "        txn_datetime = txn_date.replace(hour=hour, minute=np.random.randint(0, 60))\n",
    "\n",
    "        receipt_id = f\"R{customer_id[1:]}{i:03d}\"\n",
    "\n",
    "        basket_size = max(1, int(np.random.normal(basket_size_mean, basket_size_std)))\n",
    "\n",
    "        for _ in range(basket_size):\n",
    "            if archetype_type == \"bulk_buyer\":\n",
    "                if _ > 0 and np.random.random() < 0.4:\n",
    "                    cat_focus = transactions[-1][\"category_id\"]\n",
    "                    product_pool = products_df[products_df[\"category_id\"] == cat_focus]\n",
    "                else:\n",
    "                    product_pool = products_df\n",
    "\n",
    "            elif archetype_type == \"premium_daily\":\n",
    "                if np.random.random() < premium_prob:\n",
    "                    premium_threshold = products_df[\"standard_price\"].quantile(0.7)\n",
    "                    product_pool = products_df[products_df[\"standard_price\"] >= premium_threshold]\n",
    "                elif np.random.random() < health_prob:\n",
    "                    health_threshold = 0.6\n",
    "                    product_pool = products_df[products_df[\"health_index\"] >= health_threshold]\n",
    "                else:\n",
    "                    product_pool = products_df\n",
    "\n",
    "            elif archetype_type == \"time_saving\":\n",
    "                if np.random.random() < 0.6:\n",
    "                    product_pool = products_df[products_df[\"category_id\"].isin([\"CAT07\"])]\n",
    "                else:\n",
    "                    product_pool = products_df\n",
    "\n",
    "            else:  # 'price_sensitive'\n",
    "                if np.random.random() < 0.6:\n",
    "                    product_pool = products_df[products_df[\"brand_type\"] == \"PB\"]\n",
    "                else:\n",
    "                    product_pool = products_df\n",
    "\n",
    "            if len(product_pool) == 0:\n",
    "                product_pool = products_df\n",
    "\n",
    "            product = product_pool.sample(1).iloc[0]\n",
    "\n",
    "            base_price = product[\"standard_price\"]\n",
    "\n",
    "            discount_rate = 0\n",
    "            if np.random.random() < discount_sensitivity:\n",
    "                discount_rate = np.random.choice([0.1, 0.2, 0.3, 0.5], p=[0.4, 0.3, 0.2, 0.1])\n",
    "\n",
    "            discount_amount = int(base_price * discount_rate)\n",
    "            final_price = base_price - discount_amount\n",
    "\n",
    "            quantity = 1\n",
    "            if archetype_type == \"bulk_buyer\" and np.random.random() < 0.3:\n",
    "                quantity = np.random.randint(2, 5)\n",
    "\n",
    "            transactions.append({\n",
    "                \"customer_id\": customer_id,\n",
    "                \"purchase_datetime\": txn_datetime,\n",
    "                \"receipt_id\": receipt_id,\n",
    "                \"product_id\": product[\"product_id\"],\n",
    "                \"category_id\": product[\"category_id\"],\n",
    "                \"quantity\": quantity,\n",
    "                \"amount\": final_price * quantity,\n",
    "                \"discount_amount\": discount_amount * quantity,\n",
    "                \"standard_price\": base_price * quantity,\n",
    "            })\n",
    "\n",
    "    return transactions\n",
    "\n",
    "\n",
    "n_customers = 1000\n",
    "n_transactions_per_customer = 30\n",
    "n_products = 100\n",
    "n_categories = 10\n",
    "\n",
    "products = pd.DataFrame({\n",
    "    \"product_id\": [f\"P{i:03d}\" for i in range(1, n_products + 1)],\n",
    "    \"category_id\": [f\"CAT{np.random.randint(1, n_categories + 1):02d}\" for _ in range(n_products)],\n",
    "    \"standard_price\": np.random.randint(100, 2000, size=n_products),\n",
    "    \"brand_type\": np.random.choice([\"NB\", \"PB\"], size=n_products, p=[0.7, 0.3]),\n",
    "    \"health_index\": np.random.rand(n_products),\n",
    "})\n",
    "\n",
    "category_labels = {\n",
    "    \"CAT01\": \"Vegetables & Fruits\",\n",
    "    \"CAT02\": \"Meat & Fish\",\n",
    "    \"CAT03\": \"Dairy Products\",\n",
    "    \"CAT04\": \"Beverages\",\n",
    "    \"CAT05\": \"Confectionery\",\n",
    "    \"CAT06\": \"Condiments\",\n",
    "    \"CAT07\": \"Ready-made Foods\",\n",
    "    \"CAT08\": \"Daily Necessities\",\n",
    "    \"CAT09\": \"Health Foods\",\n",
    "    \"CAT10\": \"Other Foods\",\n",
    "}\n",
    "products[\"category_name\"] = products[\"category_id\"].map(category_labels)\n",
    "\n",
    "archetypes = [\"bulk_buyer\", \"premium_daily\", \"time_saving\", \"price_sensitive\"]\n",
    "archetype_probs = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    \"customer_id\": [f\"C{i:04d}\" for i in range(1, n_customers + 1)],\n",
    "    \"archetype\": np.random.choice(archetypes, size=n_customers, p=archetype_probs),\n",
    "})\n",
    "\n",
    "all_transactions = []\n",
    "start_date = datetime.datetime(2023, 1, 1)\n",
    "\n",
    "print(\"Generating transaction data...\")\n",
    "for idx, customer in customers.iterrows():\n",
    "    customer_txn_count = max(10, int(np.random.normal(n_transactions_per_customer, 5)))\n",
    "\n",
    "    txns = generate_customer_transactions(\n",
    "        customer[\"customer_id\"], customer_txn_count, start_date, products, customer[\"archetype\"]\n",
    "    )\n",
    "    all_transactions.extend(txns)\n",
    "\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"{idx + 1} / {len(customers)} customer data generation completed\")\n",
    "\n",
    "transactions_df = pd.DataFrame(all_transactions)\n",
    "print(f\"Generation complete: {len(transactions_df)} transactions\")\n",
    "\n",
    "print(\"\\nProducts data sample:\")\n",
    "display(products.head())\n",
    "\n",
    "print(\"\\nTransaction data sample:\")\n",
    "display(transactions_df.head())\n",
    "\n",
    "\n",
    "def extract_customer_features(transactions, customer_id):\n",
    "    customer_txn = transactions[transactions[\"customer_id\"] == customer_id]\n",
    "\n",
    "    features = {}\n",
    "    features[\"customer_id\"] = customer_id\n",
    "\n",
    "    date_range = (customer_txn[\"purchase_datetime\"].max() - customer_txn[\"purchase_datetime\"].min()).days\n",
    "    if date_range > 0:\n",
    "        features[\"purchase_frequency\"] = len(customer_txn[\"receipt_id\"].unique()) / (date_range / 30)\n",
    "    else:\n",
    "        features[\"purchase_frequency\"] = 1\n",
    "\n",
    "    receipt_amounts = customer_txn.groupby(\"receipt_id\")[\"amount\"].sum()\n",
    "    features[\"average_purchase_amount\"] = receipt_amounts.mean()\n",
    "\n",
    "    basket_sizes = customer_txn.groupby(\"receipt_id\").size()\n",
    "    features[\"average_basket_size\"] = basket_sizes.mean()\n",
    "\n",
    "    customer_txn[\"day_of_week\"] = pd.to_datetime(customer_txn[\"purchase_datetime\"]).dt.dayofweek\n",
    "    weekend_receipts = customer_txn[customer_txn[\"day_of_week\"] >= 5][\"receipt_id\"].nunique()\n",
    "    total_receipts = customer_txn[\"receipt_id\"].nunique()\n",
    "    features[\"weekend_purchase_ratio\"] = weekend_receipts / total_receipts if total_receipts > 0 else 0\n",
    "\n",
    "    customer_txn[\"hour\"] = pd.to_datetime(customer_txn[\"purchase_datetime\"]).dt.hour\n",
    "    evening_receipts = customer_txn[customer_txn[\"hour\"] >= 18][\"receipt_id\"].nunique()\n",
    "    features[\"evening_purchase_ratio\"] = evening_receipts / total_receipts if total_receipts > 0 else 0\n",
    "\n",
    "    discount_items = customer_txn[customer_txn[\"discount_amount\"] > 0]\n",
    "    features[\"discounted_item_ratio\"] = len(discount_items) / len(customer_txn) if len(customer_txn) > 0 else 0\n",
    "\n",
    "    if len(discount_items) > 0:\n",
    "        original_prices = discount_items[\"amount\"] + discount_items[\"discount_amount\"]\n",
    "        features[\"average_discount_rate\"] = (discount_items[\"discount_amount\"] / original_prices).mean()\n",
    "    else:\n",
    "        features[\"average_discount_rate\"] = 0\n",
    "\n",
    "    prepared_food = customer_txn[customer_txn[\"category_id\"] == \"CAT07\"]\n",
    "    features[\"prepared_food_ratio\"] = len(prepared_food) / len(customer_txn) if len(customer_txn) > 0 else 0\n",
    "\n",
    "    health_food = customer_txn[customer_txn[\"category_id\"] == \"CAT09\"]\n",
    "    features[\"health_food_ratio\"] = len(health_food) / len(customer_txn) if len(customer_txn) > 0 else 0\n",
    "\n",
    "    features[\"product_diversity\"] = (\n",
    "        customer_txn[\"product_id\"].nunique() / len(customer_txn) if len(customer_txn) > 0 else 0\n",
    "    )\n",
    "\n",
    "    if len(customer_txn[\"receipt_id\"].unique()) > 1:\n",
    "        purchase_dates = pd.to_datetime(customer_txn.drop_duplicates(\"receipt_id\")[\"purchase_datetime\"]).sort_values()\n",
    "        intervals = np.diff(purchase_dates) / np.timedelta64(1, \"D\")\n",
    "        features[\"purchase_interval_variability\"] = intervals.std() if len(intervals) > 0 else 0\n",
    "    else:\n",
    "        features[\"purchase_interval_variability\"] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "print(\"\\nFeature engineering in progress...\")\n",
    "customer_features_list = []\n",
    "for i, customer_id in enumerate(customers[\"customer_id\"]):\n",
    "    features = extract_customer_features(transactions_df, customer_id)\n",
    "    customer_features_list.append(features)\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"{i + 1} / {len(customers)} customer feature extraction completed\")\n",
    "\n",
    "customer_features_df = pd.DataFrame(customer_features_list)\n",
    "customer_features_df = customer_features_df.set_index(\"customer_id\")\n",
    "customer_features_df = customer_features_df.merge(customers[[\"customer_id\", \"archetype\"]], on=\"customer_id\", how=\"left\")\n",
    "\n",
    "print(\"\\nCustomer feature data sample:\")\n",
    "display(customer_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "feature_cols = [col for col in customer_features_df.columns if col not in [\"customer_id\", \"archetype\"]]\n",
    "X = customer_features_df[feature_cols]\n",
    "\n",
    "corr_matrix = X.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "threshold = 0.99\n",
    "high_corr_features = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            high_corr_features.add(corr_matrix.columns[i])\n",
    "print(f\"Features with high correlation: {high_corr_features}\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_reduced = X.drop(columns=list(high_corr_features))\n",
    "X_scaled = scaler.fit_transform(X_reduced)\n",
    "print(f\"{X.shape[0]=}, {X_reduced.shape[0]=}, {X_scaled.shape[0]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with varying numbers of archetypes\n",
    "models = {}\n",
    "for k in range(2, 11):\n",
    "    model = ImprovedArchetypalAnalysis(n_archetypes=k, max_iter=500, tol=1e-10, learning_rate=0.0001)\n",
    "    model.fit(X_scaled)\n",
    "    models[k] = model\n",
    "\n",
    "# Evaluate interpretability\n",
    "interpreter = ArchetypalAnalysisInterpreter(models)\n",
    "interpreter.evaluate_all_models(X_scaled)\n",
    "interpreter.plot_interpretability_metrics()\n",
    "\n",
    "# Suggest optimal number of archetypes\n",
    "optimal_k = interpreter.suggest_optimal_archetypes(method=\"balance\")\n",
    "print(f\"Optimal number of archetypes: {optimal_k}\")\n",
    "\n",
    "# Examine detailed interpretability of the best model\n",
    "best_model = models[optimal_k]\n",
    "best_results = interpreter.results[optimal_k]\n",
    "print(f\"Interpretability score: {best_results['interpretability_score']:.4f}\")\n",
    "print(f\"Information gain: {best_results.get('information_gain', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImprovedArchetypalAnalysis(n_archetypes=optimal_k, max_iter=5000, tol=1e-10, learning_rate=0.0001)\n",
    "model.fit(X_scaled)\n",
    "\n",
    "# Plot results\n",
    "visualizer = ArchetypalAnalysisVisualizer()\n",
    "visualizer.plot_loss(model)\n",
    "visualizer.plot_membership_weights(model, n_samples=10)\n",
    "visualizer.plot_archetype_distribution(model)\n",
    "\n",
    "if model.n_archetypes == 2:\n",
    "    visualizer.plot_archetypes_2d(model, X)\n",
    "    visualizer.plot_reconstruction_comparison(model, X)\n",
    "\n",
    "if model.n_archetypes == 3:\n",
    "    visualizer.plot_simplex_2d(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = ArchetypalAnalysisEvaluator(model)\n",
    "\n",
    "# Generate comprehensive evaluation report\n",
    "evaluator.print_evaluation_report(X)\n",
    "\n",
    "# Calculate individual evaluation metrics\n",
    "explained_var = evaluator.explained_variance(X)\n",
    "purity = evaluator.dominant_archetype_purity()\n",
    "\n",
    "# Visualize high-dimensional data\n",
    "evaluator.plot_feature_importance_heatmap(feature_names=X_reduced.columns)\n",
    "evaluator.plot_archetype_feature_comparison(top_n=10, feature_names=X_reduced.columns)\n",
    "evaluator.plot_weight_distributions(bins=100)\n",
    "evaluator.plot_distance_matrix()\n",
    "evaluator.plot_entropy_vs_reconstruction(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
