{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "\n",
    "from archetypax.models import SparseArchetypalAnalysis, ImprovedArchetypalAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "X = np.random.rand(100, 10)  # Sample data (100 samples Ã— 10 features)\n",
    "\n",
    "# Creating a sparse archetypal analysis model with L1 regularization\n",
    "model = SparseArchetypalAnalysis(\n",
    "    n_archetypes=3,         # Number of archetypes\n",
    "    lambda_sparsity=0.1,    # Strength of sparsity constraint\n",
    "    sparsity_method=\"l1\",   # Method of sparsity (\"l1\", \"l0_approx\", \"feature_selection\")\n",
    "    max_iter=200,           # Maximum number of iterations\n",
    "    normalize=True          # Normalize the data\n",
    ")\n",
    "\n",
    "# Fitting the model\n",
    "weights = model.fit_transform(X)\n",
    "\n",
    "# Utilizing the results of the training\n",
    "X_reconstructed = np.dot(weights, model.archetypes)\n",
    "sparsity_scores = model.get_archetype_sparsity() # Sparsity scores of the archetypes\n",
    "\n",
    "print(f\"Sparsity scores of the archetypes: {sparsity_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def generate_synthetic_data(n_samples=500, n_features=50, n_true_archetypes=5, noise_level=0.1):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Generate true archetypes (each archetype activates only a few features)\n",
    "    true_archetypes = np.zeros((n_true_archetypes, n_features))\n",
    "\n",
    "    for i in range(n_true_archetypes):\n",
    "        # Each archetype is associated with a specific group of features\n",
    "        features_per_archetype = n_features // n_true_archetypes\n",
    "        start_idx = i * features_per_archetype\n",
    "        end_idx = (i + 1) * features_per_archetype\n",
    "\n",
    "        # Activate only a few features (sparsity)\n",
    "        n_active = features_per_archetype // 2\n",
    "        active_indices = np.random.choice(\n",
    "            np.arange(start_idx, end_idx),\n",
    "            size=n_active,\n",
    "            replace=False\n",
    "        )\n",
    "        true_archetypes[i, active_indices] = np.random.uniform(0.5, 1.0, size=n_active)\n",
    "\n",
    "        # Add some noisy features (not completely isolated)\n",
    "        noise_indices = np.random.choice(\n",
    "            np.setdiff1d(np.arange(n_features), np.arange(start_idx, end_idx)),\n",
    "            size=features_per_archetype // 5,\n",
    "            replace=False\n",
    "        )\n",
    "        true_archetypes[i, noise_indices] = np.random.uniform(0.1, 0.3, size=len(noise_indices))\n",
    "\n",
    "    # Normalize each archetype\n",
    "    true_archetypes = true_archetypes / np.linalg.norm(true_archetypes, axis=1, keepdims=True)\n",
    "\n",
    "    # Generate weights for each data point\n",
    "    weights = np.zeros((n_samples, n_true_archetypes))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Each sample is primarily a combination of 1-2 archetypes\n",
    "        n_dominant = np.random.choice([1, 2], p=[0.7, 0.3])\n",
    "        dominant_archetypes = np.random.choice(n_true_archetypes, size=n_dominant, replace=False)\n",
    "\n",
    "        # Generate weights\n",
    "        weights_unnormalized = np.zeros(n_true_archetypes)\n",
    "        weights_unnormalized[dominant_archetypes] = np.random.uniform(0.5, 1.0, size=n_dominant)\n",
    "\n",
    "        # Assign a small amount of weight to other archetypes\n",
    "        non_dominant = np.setdiff1d(np.arange(n_true_archetypes), dominant_archetypes)\n",
    "        if len(non_dominant) > 0:\n",
    "            n_minor = np.random.randint(0, len(non_dominant) + 1)\n",
    "            if n_minor > 0:\n",
    "                minor_archetypes = np.random.choice(non_dominant, size=n_minor, replace=False)\n",
    "                weights_unnormalized[minor_archetypes] = np.random.uniform(0.01, 0.2, size=n_minor)\n",
    "\n",
    "        # Normalize weights\n",
    "        weights[i] = weights_unnormalized / weights_unnormalized.sum()\n",
    "\n",
    "    # Generate data\n",
    "    X_clean = np.dot(weights, true_archetypes)\n",
    "\n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, noise_level, size=X_clean.shape)\n",
    "    X = X_clean + noise\n",
    "\n",
    "    # Clip negative values to zero (as non-negative data)\n",
    "    X = np.maximum(X, 0)\n",
    "\n",
    "    return X, true_archetypes, weights\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Generating synthetic data\n",
    "    print(\"Generating synthetic data...\")\n",
    "    n_samples = 300\n",
    "    n_features = 50\n",
    "    n_true_archetypes = 5\n",
    "    X, true_archetypes, true_weights = generate_synthetic_data(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_true_archetypes=n_true_archetypes,\n",
    "        noise_level=0.15\n",
    "    )\n",
    "\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "\n",
    "    # Visualizing the data\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(X[:50], aspect='auto', cmap='viridis')\n",
    "    plt.title(\"Data Sample (First 50 Entries)\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(true_archetypes, aspect='auto', cmap='viridis')\n",
    "    plt.title(\"True Archetypes\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualizing with PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Color based on the true dominant archetypes\n",
    "    dominant_archetypes = np.argmax(true_weights, axis=1)\n",
    "\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dominant_archetypes, cmap='viridis', alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Dominant Archetypes')\n",
    "    plt.title(\"Synthetic Data Visualized with PCA\")\n",
    "    plt.xlabel(\"First Principal Component\")\n",
    "    plt.ylabel(\"Second Principal Component\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Comparing standard archetypal analysis with sparse archetypal analysis\n",
    "    n_archetypes = n_true_archetypes  # Set to the true number of archetypes\n",
    "\n",
    "    print(f\"\\nRunning standard archetypal analysis (number of archetypes = {n_archetypes})...\")\n",
    "    model_standard = ImprovedArchetypalAnalysis(n_archetypes=n_archetypes, random_seed=42)\n",
    "    model_standard.fit(X, normalize=True)\n",
    "\n",
    "    print(f\"\\nRunning L1 sparse archetypal analysis (number of archetypes = {n_archetypes})...\")\n",
    "    model_sparse_l1 = SparseArchetypalAnalysis(\n",
    "        n_archetypes=n_archetypes,\n",
    "        random_seed=42,\n",
    "        lambda_sparsity=0.15,\n",
    "        sparsity_method=\"l1\"\n",
    "    )\n",
    "    model_sparse_l1.fit(X, normalize=True)\n",
    "\n",
    "    print(f\"\\nRunning feature selection sparse archetypal analysis (number of archetypes = {n_archetypes})...\")\n",
    "    model_sparse_fs = SparseArchetypalAnalysis(\n",
    "        n_archetypes=n_archetypes,\n",
    "        random_seed=42,\n",
    "        lambda_sparsity=0.15,\n",
    "        sparsity_method=\"feature_selection\"\n",
    "    )\n",
    "    model_sparse_fs.fit(X, normalize=True)\n",
    "\n",
    "    # Comparing sparsity scores\n",
    "    sparsity_standard = np.zeros(n_archetypes)\n",
    "    for i in range(n_archetypes):\n",
    "        values = np.abs(model_standard.archetypes[i])\n",
    "        sorted_values = np.sort(values)\n",
    "        n = len(sorted_values)\n",
    "        cumsum = np.cumsum(sorted_values)\n",
    "        gini = 1 - 2 * np.sum(cumsum) / (n * np.sum(sorted_values))\n",
    "        sparsity_standard[i] = gini\n",
    "\n",
    "    sparsity_l1 = model_sparse_l1.get_archetype_sparsity()\n",
    "    sparsity_fs = model_sparse_fs.get_archetype_sparsity()\n",
    "\n",
    "    print(\"\\nSparsity scores of the archetypes (Gini coefficient, higher is sparser):\")\n",
    "    print(f\"Standard Model: {sparsity_standard.mean():.4f}\")\n",
    "    print(f\"L1 Sparse Model: {sparsity_l1.mean():.4f}\")\n",
    "    print(f\"Feature Selection Model: {sparsity_fs.mean():.4f}\")\n",
    "\n",
    "    # Visualizing the archetypes\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    # True archetypes\n",
    "    plt.subplot(4, 1, 1)\n",
    "    sns.heatmap(true_archetypes, cmap='viridis', cbar_kws={'label': 'Value'})\n",
    "    plt.title(\"True Archetypes\")\n",
    "    plt.ylabel(\"Archetypes\")\n",
    "\n",
    "    # Archetypes of the standard model\n",
    "    plt.subplot(4, 1, 2)\n",
    "    sns.heatmap(model_standard.archetypes, cmap='viridis', cbar_kws={'label': 'Value'})\n",
    "    plt.title(f\"Archetypes of the Standard Model (Average Sparsity: {sparsity_standard.mean():.4f})\")\n",
    "    plt.ylabel(\"Archetypes\")\n",
    "\n",
    "    # Archetypes of the L1 sparse model\n",
    "    plt.subplot(4, 1, 3)\n",
    "    sns.heatmap(model_sparse_l1.archetypes, cmap='viridis', cbar_kws={'label': 'Value'})\n",
    "    plt.title(f\"Archetypes of the L1 Sparse Model (Average Sparsity: {sparsity_l1.mean():.4f})\")\n",
    "    plt.ylabel(\"Archetypes\")\n",
    "\n",
    "    # Archetypes of the feature selection model\n",
    "    plt.subplot(4, 1, 4)\n",
    "    sns.heatmap(model_sparse_fs.archetypes, cmap='viridis', cbar_kws={'label': 'Value'})\n",
    "    plt.title(f\"Archetypes of the Feature Selection Model (Average Sparsity: {sparsity_fs.mean():.4f})\")\n",
    "    plt.ylabel(\"Archetypes\")\n",
    "    plt.xlabel(\"Features\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Comparing reconstruction errors\n",
    "    X_recon_standard = model_standard.reconstruct()\n",
    "    X_recon_sparse_l1 = model_sparse_l1.reconstruct()\n",
    "    X_recon_sparse_fs = model_sparse_fs.reconstruct()\n",
    "\n",
    "    mse_standard = np.mean((X - X_recon_standard) ** 2)\n",
    "    mse_sparse_l1 = np.mean((X - X_recon_sparse_l1) ** 2)\n",
    "    mse_sparse_fs = np.mean((X - X_recon_sparse_fs) ** 2)\n",
    "\n",
    "    print(\"\\nReconstruction Mean Squared Error (MSE):\")\n",
    "    print(f\"Standard Model: {mse_standard:.6f}\")\n",
    "    print(f\"L1 Sparse Model: {mse_sparse_l1:.6f}\")\n",
    "    print(f\"Feature Selection Model: {mse_sparse_fs:.6f}\")\n",
    "\n",
    "    # Visualizing the feature distribution of each archetype\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    for i in range(min(n_archetypes, 6)):  # Display a maximum of 6 archetypes\n",
    "        plt.subplot(3, 2, i+1)\n",
    "\n",
    "        plt.plot(true_archetypes[i], 'k-', label='True Archetype', alpha=0.7)\n",
    "        plt.plot(model_standard.archetypes[i], 'b-', label='Standard Model')\n",
    "        plt.plot(model_sparse_l1.archetypes[i], 'r-', label='L1 Sparse Model')\n",
    "        plt.plot(model_sparse_fs.archetypes[i], 'g-', label='Feature Selection Model')\n",
    "\n",
    "        plt.title(f\"Feature Distribution of Archetype {i+1}\")\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if i == 0:  # Show legend only on the first subplot\n",
    "            plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot showing the trade-off between sparsity and reconstruction error for each model\n",
    "    sparsity_values = [\n",
    "        sparsity_standard.mean(),\n",
    "        sparsity_l1.mean(),\n",
    "        sparsity_fs.mean()\n",
    "    ]\n",
    "\n",
    "    mse_values = [mse_standard, mse_sparse_l1, mse_sparse_fs]\n",
    "    model_names = ['Standard Model', 'L1 Sparse Model', 'Feature Selection Model']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(sparsity_values, mse_values, s=100)\n",
    "\n",
    "    # Label each point\n",
    "    for i, name in enumerate(model_names):\n",
    "        plt.annotate(name, (sparsity_values[i], mse_values[i]), xytext=(5, 5), textcoords='offset points',)\n",
    "\n",
    "    plt.xlabel('Average Sparsity (Gini Coefficient)')\n",
    "    plt.ylabel('Reconstruction MSE')\n",
    "    plt.title('Trade-off between Sparsity and Reconstruction Error')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
