{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial: Interpreting Biarchetypal Analysis Models\n",
    "\n",
    "This notebook demonstrates sophisticated techniques for interpreting and evaluating biarchetypal analysis models using the `BiarchetypalAnalysisInterpreter` class from the `archetypax` library. Biarchetypal analysis extends traditional archetypal analysis by simultaneously identifying archetypes in both observations (rows) and features (columns), offering a more nuanced understanding of complex data structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Essential Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import archetypax components\n",
    "from archetypax.models.biarchetypes import BiarchetypalAnalysis\n",
    "from archetypax.tools.interpret import BiarchetypalAnalysisInterpreter\n",
    "from archetypax.tools.visualization import BiarchetypalAnalysisVisualizer\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Synthetic Data with Dual Structure\n",
    "\n",
    "We'll create a synthetic dataset with inherent structure in both rows and columns, making it ideal for demonstrating biarchetypal analysis. This approach allows us to evaluate the model's ability to recover known patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define dataset parameters\n",
    "n_samples = 10\n",
    "n_features = 10\n",
    "n_row_clusters = 3\n",
    "n_col_clusters = 4\n",
    "\n",
    "# Generate data with row clusters\n",
    "X_raw, row_labels = make_blobs(\n",
    "    n_samples=n_samples, n_features=n_features, centers=n_row_clusters, random_state=42, cluster_std=1.5\n",
    ")\n",
    "\n",
    "# Add column structure by creating feature groups with distinct correlation patterns\n",
    "feature_groups = np.array_split(np.arange(n_features), n_col_clusters)\n",
    "col_labels = np.zeros(n_features, dtype=int)\n",
    "\n",
    "# Apply different correlation structures to each feature group\n",
    "for i, group in enumerate(feature_groups):\n",
    "    # Generate a random positive-definite correlation matrix\n",
    "    corr_matrix = np.random.uniform(0.5, 0.9, size=(len(group), len(group)))\n",
    "    corr_matrix = (corr_matrix + corr_matrix.T) / 2  # Ensure symmetry\n",
    "    np.fill_diagonal(corr_matrix, 1.0)  # Set diagonal to 1\n",
    "\n",
    "    # Apply correlation structure using Cholesky decomposition\n",
    "    L = np.linalg.cholesky(corr_matrix)\n",
    "    uncorrelated = X_raw[:, group]\n",
    "    correlated = uncorrelated @ L.T\n",
    "\n",
    "    # Apply group-specific scaling\n",
    "    scale_factor = np.random.uniform(0.5, 2.0)\n",
    "    X_raw[:, group] = correlated * scale_factor\n",
    "\n",
    "    # Assign column labels\n",
    "    col_labels[group] = i\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_raw)\n",
    "\n",
    "print(f\"Dataset dimensions: {X.shape}\")\n",
    "print(f\"Number of row clusters: {n_row_clusters}\")\n",
    "print(f\"Number of column clusters: {n_col_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Dual Structure\n",
    "\n",
    "Before applying biarchetypal analysis, let's visualize the inherent structure in our synthetic dataset to establish a baseline for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the data matrix with rows and columns sorted by their respective clusters\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Sort indices by cluster labels\n",
    "row_idx = np.argsort(row_labels)\n",
    "col_idx = np.argsort(col_labels)\n",
    "\n",
    "# Generate heatmap with sorted data\n",
    "sorted_data = X[row_idx][:, col_idx]\n",
    "sns.heatmap(sorted_data, cmap=\"viridis\", center=0)\n",
    "plt.title(\"Data Matrix Heatmap (Sorted by True Cluster Labels)\")\n",
    "plt.xlabel(\"Features (Columns)\")\n",
    "plt.ylabel(\"Samples (Rows)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Multiple Biarchetypal Models\n",
    "\n",
    "We'll train a suite of biarchetypal models with varying numbers of row and column archetypes to identify the optimal configuration for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of archetype numbers to explore\n",
    "row_archetypes_range = range(2, 6)  # From 2 to 5 row archetypes\n",
    "col_archetypes_range = range(2, 6)  # From 2 to 5 column archetypes\n",
    "\n",
    "# Dictionary to store trained models\n",
    "models_dict = {}\n",
    "\n",
    "# Train models for each combination of row and column archetypes\n",
    "for n_row in row_archetypes_range:\n",
    "    for n_col in col_archetypes_range:\n",
    "        print(f\"Training model with {n_row} row archetypes and {n_col} column archetypes...\")\n",
    "\n",
    "        # Initialize the model with carefully selected hyperparameters\n",
    "        model = BiarchetypalAnalysis(\n",
    "            n_row_archetypes=n_row,\n",
    "            n_col_archetypes=n_col,\n",
    "            max_iter=100,  # Limit iterations for faster convergence\n",
    "            random_seed=42,\n",
    "            learning_rate=0.01,\n",
    "            lambda_reg=0.01,  # Regularization parameter for improved stability\n",
    "        )\n",
    "\n",
    "        # Fit the model to our data\n",
    "        model.fit(X)\n",
    "\n",
    "        # Store the trained model\n",
    "        models_dict[(n_row, n_col)] = model\n",
    "\n",
    "print(\"Model training complete for all archetype combinations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initializing and Evaluating with BiarchetypalAnalysisInterpreter\n",
    "\n",
    "Now we'll leverage the `BiarchetypalAnalysisInterpreter` to systematically evaluate our trained models using multiple interpretability metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the interpreter\n",
    "interpreter = BiarchetypalAnalysisInterpreter()\n",
    "\n",
    "# Add all trained models to the interpreter\n",
    "for (n_row, n_col), model in models_dict.items():\n",
    "    interpreter.add_model(n_row, n_col, model)\n",
    "\n",
    "# Evaluate all models using comprehensive metrics\n",
    "results = interpreter.evaluate_all_models(X)\n",
    "\n",
    "# Calculate information gain for model comparison\n",
    "interpreter.compute_information_gain(X)\n",
    "\n",
    "print(\"Comprehensive evaluation completed for all models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Determining Optimal Archetype Configurations\n",
    "\n",
    "We'll employ multiple methodologies to identify the optimal number of archetypes, comparing their recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal configuration using balance method\n",
    "optimal_balance = interpreter.suggest_optimal_biarchetypes(method=\"balance\")\n",
    "print(\n",
    "    f\"Optimal configuration via balance method: {optimal_balance[0]} row archetypes, {optimal_balance[1]} column archetypes\"\n",
    ")\n",
    "\n",
    "# Determine optimal configuration using interpretability method\n",
    "optimal_interpretability = interpreter.suggest_optimal_biarchetypes(method=\"interpretability\")\n",
    "print(\n",
    "    f\"Optimal configuration via interpretability method: {optimal_interpretability[0]} row archetypes, {optimal_interpretability[1]} column archetypes\"\n",
    ")\n",
    "\n",
    "# Determine optimal configuration using information gain method\n",
    "try:\n",
    "    optimal_info_gain = interpreter.suggest_optimal_biarchetypes(method=\"information_gain\")\n",
    "    print(\n",
    "        f\"Optimal configuration via information gain method: {optimal_info_gain[0]} row archetypes, {optimal_info_gain[1]} column archetypes\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Information gain method unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Interpretability Metrics\n",
    "\n",
    "Let's create heatmaps to visualize how interpretability metrics vary across different archetype configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate interpretability heatmaps\n",
    "fig = interpreter.plot_interpretability_heatmap()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. In-Depth Analysis of the Optimal Model\n",
    "\n",
    "Now we'll conduct a detailed examination of the model identified as optimal by our evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal model (using the balance method recommendation)\n",
    "optimal_model = models_dict[optimal_balance]\n",
    "\n",
    "# Display comprehensive evaluation metrics for the optimal model\n",
    "optimal_results = results[optimal_balance]\n",
    "print(\"Evaluation metrics for the optimal model:\")\n",
    "for metric, value in optimal_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyzing Feature Distinctiveness and Sparsity\n",
    "\n",
    "We'll examine how distinctive and sparse each archetype is, providing insights into their interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract row and column archetypes from the optimal model\n",
    "row_archetypes, col_archetypes = optimal_model.get_all_archetypes()\n",
    "\n",
    "# Calculate feature distinctiveness for row archetypes\n",
    "row_distinctiveness = interpreter.feature_distinctiveness(row_archetypes)\n",
    "print(\"Feature distinctiveness scores for row archetypes:\")\n",
    "for i, score in enumerate(row_distinctiveness):\n",
    "    print(f\"Archetype {i + 1}: {score:.4f}\")\n",
    "\n",
    "# Calculate feature distinctiveness for column archetypes\n",
    "col_distinctiveness = interpreter.feature_distinctiveness(col_archetypes)\n",
    "print(\"\\nFeature distinctiveness scores for column archetypes:\")\n",
    "for i, score in enumerate(col_distinctiveness):\n",
    "    print(f\"Archetype {i + 1}: {score:.4f}\")\n",
    "\n",
    "# Calculate sparsity coefficients for row archetypes\n",
    "row_sparsity = interpreter.sparsity_coefficient(row_archetypes)\n",
    "print(\"\\nSparsity coefficients for row archetypes:\")\n",
    "for i, score in enumerate(row_sparsity):\n",
    "    print(f\"Archetype {i + 1}: {score:.4f}\")\n",
    "\n",
    "# Calculate sparsity coefficients for column archetypes\n",
    "col_sparsity = interpreter.sparsity_coefficient(col_archetypes)\n",
    "print(\"\\nSparsity coefficients for column archetypes:\")\n",
    "for i, score in enumerate(col_sparsity):\n",
    "    print(f\"Archetype {i + 1}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluating Cluster Purity\n",
    "\n",
    "We'll assess how well the identified archetypes correspond to natural clusters in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights for rows and columns\n",
    "row_weights, col_weights = optimal_model.get_all_weights()\n",
    "\n",
    "# Calculate cluster purity for row weights\n",
    "row_dominant, row_purity = interpreter.cluster_purity(row_weights)\n",
    "print(f\"Cluster purity for row archetypes: {row_purity:.4f}\")\n",
    "\n",
    "# Calculate cluster purity for column weights\n",
    "col_dominant, col_purity = interpreter.cluster_purity(col_weights)\n",
    "print(f\"Cluster purity for column archetypes: {col_purity:.4f}\")\n",
    "\n",
    "# Visualize the distribution of dominant archetypes\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=row_dominant)\n",
    "plt.title(\"Distribution of Dominant Row Archetypes\")\n",
    "plt.xlabel(\"Archetype ID\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=col_dominant)\n",
    "plt.title(\"Distribution of Dominant Column Archetypes\")\n",
    "plt.xlabel(\"Archetype ID\")\n",
    "plt.ylabel(\"Number of Features\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Visualization of Biarchetypal Structure\n",
    "\n",
    "We'll employ the `BiarchetypalAnalysisVisualizer` to create sophisticated visualizations of our model's results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the visualizer with our optimal model\n",
    "visualizer = BiarchetypalAnalysisVisualizer()\n",
    "\n",
    "# Generate dual membership heatmap\n",
    "visualizer.plot_dual_membership_heatmap(model=optimal_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparing with Ground Truth\n",
    "\n",
    "Finally, we'll compare the archetypes identified by our model with the known ground truth clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dominant row archetypes with true row cluster labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=row_dominant, cmap=\"viridis\", s=100, alpha=0.7, edgecolors=\"k\")\n",
    "plt.title(\"Data Points Colored by Dominant Row Archetype\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.colorbar(label=\"Archetype ID\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=row_labels, cmap=\"plasma\", s=100, alpha=0.7, edgecolors=\"k\")\n",
    "plt.title(\"Data Points Colored by True Cluster Label\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.colorbar(label=\"Cluster ID\")\n",
    "plt.show()\n",
    "\n",
    "# Compare dominant column archetypes with true column cluster labels\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot dominant column archetypes\n",
    "ax1.bar(range(n_features), col_dominant)\n",
    "ax1.set_title(\"Dominant Column Archetypes\")\n",
    "ax1.set_xlabel(\"Feature Index\")\n",
    "ax1.set_ylabel(\"Archetype ID\")\n",
    "\n",
    "# Plot true column cluster labels\n",
    "ax2.bar(range(n_features), col_labels)\n",
    "ax2.set_title(\"True Column Cluster Labels\")\n",
    "ax2.set_xlabel(\"Feature Index\")\n",
    "ax2.set_ylabel(\"Cluster ID\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion and Key Insights\n",
    "\n",
    "This tutorial has demonstrated the sophisticated capabilities of the `BiarchetypalAnalysisInterpreter` for evaluating and interpreting biarchetypal models. Key aspects covered include:\n",
    "\n",
    "1. Generation of synthetic data with dual structure in both rows and columns\n",
    "2. Training of multiple biarchetypal models with varying archetype configurations\n",
    "3. Systematic evaluation and selection of optimal archetype numbers\n",
    "4. Visualization of interpretability metrics across model configurations\n",
    "5. In-depth analysis of feature distinctiveness and sparsity\n",
    "6. Assessment of cluster purity and archetype distribution\n",
    "7. Advanced visualization of biarchetypal structures\n",
    "8. Comparison with ground truth to validate model performance\n",
    "\n",
    "Biarchetypal analysis offers a powerful approach for simultaneously capturing structure in both observations and features, with the `BiarchetypalAnalysisInterpreter` providing essential tools for model interpretation and evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
