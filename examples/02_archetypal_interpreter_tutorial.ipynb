{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial: Interpreting Archetypal Analysis Models\n",
    "\n",
    "This notebook demonstrates sophisticated techniques for interpreting and evaluating archetypal analysis models using the `ArchetypalAnalysisInterpreter` class from the `archetypax` library. Archetypal analysis identifies extreme patterns (archetypes) in data, and the interpreter provides quantitative measures to assess model quality and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Essential Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import archetypax components\n",
    "from archetypax.models.archetypes import ImprovedArchetypalAnalysis\n",
    "from archetypax.tools.interpret import ArchetypalAnalysisInterpreter\n",
    "from archetypax.tools.visualization import ArchetypalAnalysisVisualizer\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Synthetic Data\n",
    "\n",
    "We'll create a synthetic dataset with clear cluster structure to demonstrate archetypal analysis. This controlled environment allows us to evaluate how well the model identifies meaningful archetypes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define dataset parameters\n",
    "n_samples = 200\n",
    "n_features = 10\n",
    "n_clusters = 4\n",
    "\n",
    "# Generate data with clear cluster structure\n",
    "X_raw, y_true = make_blobs(\n",
    "    n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=42, cluster_std=1.2\n",
    ")\n",
    "\n",
    "# Add some noise features to make the problem more challenging\n",
    "noise_features = np.random.normal(0, 0.5, size=(n_samples, 5))\n",
    "X_raw = np.hstack([X_raw, noise_features])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_raw)\n",
    "\n",
    "print(f\"Dataset dimensions: {X.shape}\")\n",
    "print(f\"Number of true clusters: {n_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Data Structure\n",
    "\n",
    "Before applying archetypal analysis, let's visualize the inherent structure in our synthetic dataset to establish a baseline for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of the first two dimensions colored by true cluster\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap=\"viridis\", s=80, alpha=0.8, edgecolors=\"k\")\n",
    "plt.title(\"Data Visualization (First Two Dimensions)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.colorbar(label=\"True Cluster\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Create a correlation heatmap to visualize feature relationships\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = np.corrcoef(X.T)\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Multiple Archetypal Analysis Models\n",
    "\n",
    "We'll train a series of archetypal analysis models with varying numbers of archetypes to identify the optimal configuration for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of archetype numbers to explore\n",
    "archetype_range = range(2, 9)  # From 2 to 8 archetypes\n",
    "\n",
    "# Dictionary to store trained models\n",
    "models_dict = {}\n",
    "\n",
    "# Train models for each number of archetypes\n",
    "for n_archetypes in archetype_range:\n",
    "    print(f\"Training model with {n_archetypes} archetypes...\")\n",
    "\n",
    "    # Initialize the model with carefully selected hyperparameters\n",
    "    model = ImprovedArchetypalAnalysis(\n",
    "        n_archetypes=n_archetypes,\n",
    "        max_iter=200,  # Sufficient iterations for convergence\n",
    "        random_seed=42,\n",
    "        learning_rate=0.01,\n",
    "    )\n",
    "\n",
    "    # Fit the model to our data\n",
    "    model.fit(X)\n",
    "\n",
    "    # Store the trained model\n",
    "    models_dict[n_archetypes] = model\n",
    "\n",
    "print(\"Model training complete for all archetype configurations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initializing and Evaluating with ArchetypalAnalysisInterpreter\n",
    "\n",
    "Now we'll leverage the `ArchetypalAnalysisInterpreter` to systematically evaluate our trained models using multiple interpretability metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the interpreter\n",
    "interpreter = ArchetypalAnalysisInterpreter()\n",
    "\n",
    "# Add all trained models to the interpreter\n",
    "for n_archetypes, model in models_dict.items():\n",
    "    interpreter.add_model(n_archetypes, model)\n",
    "\n",
    "# Evaluate all models using comprehensive metrics\n",
    "results = interpreter.evaluate_all_models(X)\n",
    "\n",
    "print(\"Comprehensive evaluation completed for all models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing Interpretability Metrics\n",
    "\n",
    "Let's examine how interpretability metrics vary across different numbers of archetypes to identify the optimal configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for visualization\n",
    "n_archetypes_list = sorted(results.keys())\n",
    "distinctiveness_scores = [results[k][\"avg_distinctiveness\"] for k in n_archetypes_list]\n",
    "sparsity_scores = [results[k][\"avg_sparsity\"] for k in n_archetypes_list]\n",
    "purity_scores = [results[k][\"avg_purity\"] for k in n_archetypes_list]\n",
    "interpretability_scores = [results[k][\"interpretability_score\"] for k in n_archetypes_list]\n",
    "\n",
    "# Create a multi-metric plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(n_archetypes_list, distinctiveness_scores, \"o-\", label=\"Distinctiveness\", linewidth=2)\n",
    "plt.plot(n_archetypes_list, sparsity_scores, \"s-\", label=\"Sparsity\", linewidth=2)\n",
    "plt.plot(n_archetypes_list, purity_scores, \"^-\", label=\"Purity\", linewidth=2)\n",
    "plt.plot(n_archetypes_list, interpretability_scores, \"D-\", label=\"Overall Interpretability\", linewidth=3)\n",
    "\n",
    "plt.xlabel(\"Number of Archetypes\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Interpretability Metrics by Number of Archetypes\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xticks(n_archetypes_list)\n",
    "plt.show()\n",
    "\n",
    "# Identify the optimal number of archetypes based on interpretability\n",
    "optimal_n_archetypes = n_archetypes_list[np.argmax(interpretability_scores)]\n",
    "print(f\"Optimal number of archetypes based on interpretability: {optimal_n_archetypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reconstruction Error Analysis\n",
    "\n",
    "Let's also examine how reconstruction error varies with the number of archetypes to balance interpretability with model fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction error for each model\n",
    "reconstruction_errors = []\n",
    "\n",
    "for n_archetypes in n_archetypes_list:\n",
    "    model = models_dict[n_archetypes]\n",
    "    X_reconstructed = model.reconstruct(X)\n",
    "    error = np.mean(np.sum((X - X_reconstructed) ** 2, axis=1))\n",
    "    reconstruction_errors.append(error)\n",
    "\n",
    "# Plot reconstruction error\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_archetypes_list, reconstruction_errors, \"o-\", color=\"crimson\", linewidth=2, markersize=10)\n",
    "plt.xlabel(\"Number of Archetypes\")\n",
    "plt.ylabel(\"Mean Squared Reconstruction Error\")\n",
    "plt.title(\"Reconstruction Error by Number of Archetypes\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(n_archetypes_list)\n",
    "plt.show()\n",
    "\n",
    "# Calculate elbow point (where adding more archetypes yields diminishing returns)\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# Function to find the point of maximum curvature (elbow point)\n",
    "def find_elbow_point(x, y):\n",
    "    # Normalize data\n",
    "    x_norm = (x - min(x)) / (max(x) - min(x))\n",
    "    y_norm = (y - min(y)) / (max(y) - min(y))\n",
    "\n",
    "    # Create interpolation function\n",
    "    interp_func = interp1d(x_norm, y_norm, kind=\"cubic\")\n",
    "\n",
    "    # Function to minimize (distance from point to line connecting endpoints)\n",
    "    def distance_to_line(point):\n",
    "        p = np.array([point, interp_func(point)])\n",
    "        start = np.array([0, interp_func(0)])\n",
    "        end = np.array([1, interp_func(1)])\n",
    "        return np.abs(np.cross(end - start, start - p)) / np.linalg.norm(end - start)\n",
    "\n",
    "    # Find point of maximum distance\n",
    "    result = minimize(lambda p: -distance_to_line(p[0]), [0.5], bounds=[(0, 1)])\n",
    "    elbow_x = result.x[0] * (max(x) - min(x)) + min(x)\n",
    "    return int(round(elbow_x))\n",
    "\n",
    "\n",
    "# Find elbow point\n",
    "elbow_n_archetypes = find_elbow_point(np.array(n_archetypes_list), np.array(reconstruction_errors))\n",
    "print(f\"Optimal number of archetypes based on elbow method: {elbow_n_archetypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. In-Depth Analysis of the Optimal Model\n",
    "\n",
    "Now we'll conduct a detailed examination of the model identified as optimal by our evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal model based on interpretability\n",
    "optimal_model = models_dict[optimal_n_archetypes]\n",
    "\n",
    "# Display comprehensive evaluation metrics for the optimal model\n",
    "optimal_results = results[optimal_n_archetypes]\n",
    "print(f\"Detailed evaluation metrics for model with {optimal_n_archetypes} archetypes:\")\n",
    "for metric, value in optimal_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    elif isinstance(value, np.ndarray) and value.ndim == 1:\n",
    "        print(f\"\\n{metric} per archetype:\")\n",
    "        for i, val in enumerate(value):\n",
    "            print(f\"  Archetype {i + 1}: {val:.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyzing Feature Distinctiveness and Sparsity\n",
    "\n",
    "We'll examine how distinctive and sparse each archetype is, providing insights into their interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract archetypes from the optimal model\n",
    "archetypes = np.asarray(optimal_model.archetypes)\n",
    "\n",
    "# Calculate feature distinctiveness for archetypes\n",
    "distinctiveness = interpreter.feature_distinctiveness(archetypes)\n",
    "print(\"Feature distinctiveness scores for archetypes:\")\n",
    "for i, score in enumerate(distinctiveness):\n",
    "    print(f\"Archetype {i + 1}: {score:.4f}\")\n",
    "\n",
    "# Calculate sparsity coefficients for archetypes\n",
    "sparsity = interpreter.sparsity_coefficient(archetypes)\n",
    "print(\"\\nSparsity coefficients for archetypes:\")\n",
    "for i, score in enumerate(sparsity):\n",
    "    print(f\"Archetype {i + 1}: {score:.4f}\")\n",
    "\n",
    "# Visualize archetype profiles\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i in range(archetypes.shape[0]):\n",
    "    plt.subplot(int(np.ceil(archetypes.shape[0] / 2)), 2, i + 1)\n",
    "    plt.bar(range(archetypes.shape[1]), archetypes[i], color=\"teal\", alpha=0.7)\n",
    "    plt.title(f\"Archetype {i + 1} Profile\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluating Cluster Purity\n",
    "\n",
    "We'll assess how well the identified archetypes correspond to natural clusters in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from the optimal model\n",
    "weights = np.asarray(optimal_model.weights)\n",
    "\n",
    "# Calculate cluster purity\n",
    "purity_scores, avg_purity = interpreter.cluster_purity(weights)\n",
    "print(f\"Average cluster purity: {avg_purity:.4f}\")\n",
    "print(\"\\nPurity scores per archetype:\")\n",
    "for i, score in enumerate(purity_scores):\n",
    "    print(f\"Archetype {i + 1}: {score:.4f}\")\n",
    "\n",
    "# Identify dominant archetype for each sample\n",
    "dominant_archetypes = np.argmax(weights, axis=1)\n",
    "\n",
    "# Visualize the distribution of dominant archetypes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=dominant_archetypes, palette=\"viridis\")\n",
    "plt.title(\"Distribution of Dominant Archetypes\")\n",
    "plt.xlabel(\"Archetype ID\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Visualization of Archetypal Structure\n",
    "\n",
    "We'll employ the `ArchetypalAnalysisVisualizer` to create sophisticated visualizations of our model's results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the visualizer with our optimal model\n",
    "visualizer = ArchetypalAnalysisVisualizer()\n",
    "\n",
    "# Generate simplex visualization (if data is low-dimensional or can be projected)\n",
    "try:\n",
    "    fig = visualizer.plot_simplex_2d(optimal_model, X)\n",
    "    plt.title(\"2D Simplex Visualization\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate simplex visualization: {e}\")\n",
    "\n",
    "# Generate archetype profiles\n",
    "fig = visualizer.plot_archetype_profiles(optimal_model, X)\n",
    "plt.title(\"Archetype Profiles\")\n",
    "plt.show()\n",
    "\n",
    "# Generate membership heatmap\n",
    "fig = visualizer.plot_membership_heatmap(optimal_model, X)\n",
    "plt.title(\"Membership Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparing with Ground Truth\n",
    "\n",
    "Finally, we'll compare the archetypes identified by our model with the known ground truth clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dominant archetypes with true cluster labels\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=dominant_archetypes, cmap=\"viridis\", s=80, alpha=0.8, edgecolors=\"k\")\n",
    "plt.title(\"Data Points Colored by Dominant Archetype\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.colorbar(label=\"Archetype ID\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap=\"plasma\", s=80, alpha=0.8, edgecolors=\"k\")\n",
    "plt.title(\"Data Points Colored by True Cluster Label\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.colorbar(label=\"Cluster ID\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate agreement between dominant archetypes and true clusters\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "ari = adjusted_rand_score(y_true, dominant_archetypes)\n",
    "nmi = normalized_mutual_info_score(y_true, dominant_archetypes)\n",
    "\n",
    "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information: {nmi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion and Key Insights\n",
    "\n",
    "This tutorial has demonstrated the sophisticated capabilities of the `ArchetypalAnalysisInterpreter` for evaluating and interpreting archetypal analysis models. Key aspects covered include:\n",
    "\n",
    "1. Generation of synthetic data with clear cluster structure\n",
    "2. Training of multiple archetypal analysis models with varying numbers of archetypes\n",
    "3. Systematic evaluation of interpretability metrics across model configurations\n",
    "4. Identification of optimal archetype numbers using both interpretability and elbow method\n",
    "5. In-depth analysis of feature distinctiveness and sparsity\n",
    "6. Assessment of cluster purity and archetype distribution\n",
    "7. Advanced visualization of archetypal structures\n",
    "8. Comparison with ground truth to validate model performance\n",
    "\n",
    "Archetypal analysis offers a powerful approach for identifying extreme patterns in data, with the `ArchetypalAnalysisInterpreter` providing essential tools for model interpretation and evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
